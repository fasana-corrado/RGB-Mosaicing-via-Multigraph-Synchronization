{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47dad686-2f65-4523-b8e9-89c0520e65e4",
   "metadata": {},
   "source": [
    "# Benchmark\n",
    "This notebook contains the code that is used to evaluate quantitatively and visualize the results of the analyzed methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94a1bed-2076-4d4d-9873-5b33bed4b32a",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a08df64-82ce-4554-a5f3-d64e097ee089",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "from math import ceil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import ipynb.fs.defs.FeatureMatching as FeatureMatching\n",
    "import ipynb.fs.defs.GraphBuilding as GraphBuilding\n",
    "import ipynb.fs.defs.SimpleGraphStitching as SimpleGraphStitching\n",
    "import ipynb.fs.defs.Utils as Utils\n",
    "from IPython.display import display\n",
    "import IPython"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7351a2-9755-4514-9e51-bf9051fd04f0",
   "metadata": {},
   "source": [
    "## Variables definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b51a0ac9-d52f-453f-90f4-d0866a2ffc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BENCHMARK_DIR = \"benchmark\" #Directory where to save the results\n",
    "FILENAME = \"results.csv\" #Name of the file that will contain the results\n",
    "INDEX = [\"number_of_matches\", \"noise_std\"] #Set of attributes that is used as index in the results file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bae759c-3a7f-4f82-baa0-10acf94ee782",
   "metadata": {},
   "source": [
    "## Functions definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6767adad-acae-4b01-abfc-bd0b1da13d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function allows to estimate the error between two homographies\n",
    "def compute_error(H_gt, #Ground truth homography\n",
    "                  H #Estimated homography\n",
    "                 ):\n",
    "    #Compute the determinant of the ground truth homography and normalize it so that the determinant is unitary\n",
    "    det_H_gt = np.linalg.det(H_gt)\n",
    "    H_gt = H_gt/np.cbrt(det_H_gt)\n",
    "    \n",
    "    #Compute the determinant of the estimated homography and normalize it so that the determinant is unitary\n",
    "    det_H = np.linalg.det(H)\n",
    "    H = H/np.cbrt(det_H)\n",
    "    \n",
    "    return np.linalg.norm(np.eye(3)-H_gt@np.linalg.inv(H)) #Compute and return the error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fad5411-fcdf-49a8-a9b4-f9fae3738c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function allows to compute the average error given a set of estimated homographies and ground truth homographies\n",
    "def compute_avg_error(H_gt, #Ground truth homography\n",
    "                      H #Estimated homography\n",
    "                     ):\n",
    "    return np.mean([compute_error(H_gt_node, H_node) for H_gt_node, H_node in zip(H_gt,H)]) #Return average error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "348d4974-5685-4d72-ba2f-9cdfa58c1499",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function allows to compute a ground truth set of homographies associated to a stitched image\n",
    "def compute_ground_truth(dataset_name, #Name of the dataset to be used\n",
    "                         imgs, #Images to be stitched\n",
    "                         T_norm, #Normalization matrix\n",
    "                         matching_threshold, #Threshold used to compute matches\n",
    "                         matches_th, #Threshold used to compute good matches\n",
    "                         idx_ref #Index of the reference image\n",
    "                        ):\n",
    "    #Perform feature matching\n",
    "    matches_dict_gt, _ = FeatureMatching.get_feature_matches(dataset_name,\n",
    "                        imgs,\n",
    "                        T_norm,\n",
    "                        matching_threshold = matching_threshold,\n",
    "                        number_of_matches = 1,\n",
    "                        matches_th = matches_th,\n",
    "                        RANSACmaxIters = 2000,\n",
    "                        save_output = False,\n",
    "                        save_images = False,\n",
    "                        noise_type = Utils.NoiseType.NO_NOISE,\n",
    "                        verbose = False\n",
    "                       )\n",
    "\n",
    "    #Produce the ground truth using simple graph stitching in a noise-free scenario\n",
    "    #Compute matrices needed to perform simple graph stitching\n",
    "    M_gt, _, _ = GraphBuilding.build_simple_graph_matrices(dataset_name,\n",
    "                imgs,\n",
    "                matches_dict_gt,\n",
    "                verbose = False,\n",
    "                save_output = False\n",
    "               )\n",
    "\n",
    "    #Apply simple graph stitching to get the ground truth homographies and the stitched image\n",
    "    H_gt, stitched_image_gt,_ = SimpleGraphStitching.simple_graph_stitching(dataset_name,\n",
    "                                imgs,\n",
    "                                T_norm,\n",
    "                                M_gt, \n",
    "                                idx_ref = idx_ref,\n",
    "                                verbose = False,\n",
    "                                save_output = False,\n",
    "                                beautify = True,\n",
    "                                warp_shape = [10000,10000])\n",
    "    \n",
    "    return H_gt, stitched_image_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "038fc9e7-a710-4073-a63f-9da3b06e1a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function allows to visualize the statistics related to the results\n",
    "def visualize_stats(df):\n",
    "    #Sort results according to the index\n",
    "    df.sort_values(by=INDEX, ascending=True, inplace=True)\n",
    "    display(df)\n",
    "    \n",
    "    #Drop the column related to the number of experiments since it's not needed\n",
    "    df_v = df.drop(columns=\"Experiments\")\n",
    "    \n",
    "    noises = np.unique([ v[1] for v in df_v.index.values]) #Set of noise values\n",
    "    matches = np.unique([ v[0] for v in df_v.index.values]) #Set of multi-edge degree values\n",
    "\n",
    "    #Plot representations\n",
    "    Utils.scatter_with_slider(noises, df_v.reorder_levels(['noise_std','number_of_matches']).sort_index(), \"Multi-edge degree\", \"Noise std\")\n",
    "    Utils.scatter_with_slider(matches, df_v, \"Noise std\", \"Multi-edge degree\")\n",
    "    Utils.mesh_plot(df_v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1b0273e-e877-4072-865f-171901affd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function allows to visualize the qualitative and quantitative results\n",
    "def visualize_results(df, results, stitched_img_gt):\n",
    "    #Plot the obtained stitched images\n",
    "    Utils.plot_images(results, stitched_img_gt)\n",
    "    \n",
    "    #Plot qualitative results\n",
    "    visualize_stats(df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fe2fdb1-a905-423f-9e30-e8df85c692fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function allows to load the results obtained for a specific experiment\n",
    "def load_gt_and_stats(dataset_dir, #Directory of the dataset to be used\n",
    "                df_file, #Name of the file that contains the results\n",
    "                dataset_name, #Name of the dataset to be used\n",
    "                imgs, #Images to be stitched\n",
    "                T_norm, #Normalization matrix\n",
    "                matching_threshold, #Threshold used to compute matches\n",
    "                matches_th, #Threshold used to compute good matches\n",
    "                idx_ref, #Index of the reference image\n",
    "                results #List of dictionaries containing the results for each method\n",
    "                ):\n",
    "    \n",
    "    index = INDEX\n",
    "    #Define where to save ground truth data\n",
    "    gt_file = os.path.join(dataset_dir,\"gt.npy\")\n",
    "    gt_img_file = os.path.join(dataset_dir,\"gt_img.jpg\")\n",
    "    \n",
    "    #Load the GT if it exists or compute it\n",
    "    if os.path.isfile(gt_file):\n",
    "        H_gt = np.load(gt_file)\n",
    "        stitched_img_gt = cv2.cvtColor(cv2.imread(gt_img_file), cv2.COLOR_BGR2RGB)\n",
    "        df = pd.read_csv(df_file, index_col = index)\n",
    "    else:\n",
    "        H_gt, stitched_img_gt = compute_ground_truth(dataset_name,imgs,T_norm,matching_threshold,matches_th, idx_ref)\n",
    "        cv2.imwrite(gt_img_file, cv2.cvtColor(stitched_img_gt, cv2.COLOR_RGB2BGR))\n",
    "        np.save(gt_file,H_gt)\n",
    "        columns = index + [r[\"name\"] for r in results] + [\"Experiments\"]\n",
    "        df = pd.DataFrame(columns=columns)\n",
    "        df.set_index(index,inplace=True)\n",
    "        \n",
    "    return H_gt, stitched_img_gt, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69e93d2a-bf16-4394-9b04-89531482d20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function allows to compute the statistics of a certain experiment\n",
    "def run_benchmark(dataset_name, #Name of the dataset to be used\n",
    "                  imgs, #Images to be stitched\n",
    "                  T_norm, #Normalization matrix\n",
    "                  matching_threshold,  #Threshold used to compute matches\n",
    "                  matches_th, #Threshold used to compute good matches\n",
    "                  idx_ref, #Index of the reference image\n",
    "                  results, #List of dictionaries containing the results for each method\n",
    "                  number_of_matches, #Multi-edge degree value\n",
    "                  noise_std, #Standard deviation of the noise\n",
    "                  noise_type, #Type of noise added during the experiments\n",
    "                  visualize = True #If True allows to plot the results of the experiment\n",
    "                 ):\n",
    "    \n",
    "    benchmark_dir = BENCHMARK_DIR\n",
    "    filename = FILENAME\n",
    "    \n",
    "    #Compute the dataset directory path\n",
    "    dataset_dir = os.path.join(benchmark_dir,f\"{dataset_name}_{idx_ref}_{matching_threshold}_{matches_th}_{noise_type.name}\")\n",
    "  \n",
    "    #Compute the path of the file where to save results\n",
    "    df_file = os.path.join(dataset_dir, filename)\n",
    "    os.makedirs(dataset_dir, exist_ok = True)\n",
    "    \n",
    "    #Load the results of the experiments\n",
    "    H_gt, stitched_img_gt, df = load_gt_and_stats(dataset_dir,\n",
    "                df_file,\n",
    "                dataset_name,\n",
    "                imgs,\n",
    "                T_norm,\n",
    "                matching_threshold,\n",
    "                matches_th, \n",
    "                idx_ref,\n",
    "                results)\n",
    "    \n",
    "    #Average error of the experiments with the same noise std and multi-edge degree\n",
    "    index = (number_of_matches, noise_std)\n",
    "    if index in df.index:\n",
    "        #Update number of experiments\n",
    "        exp = df.loc[index,'Experiments'] + 1\n",
    "        #Update average considering the new experiment's results\n",
    "        new_row = [df.loc[index,r[\"name\"]] * ((exp-1)/exp) + compute_avg_error(H_gt,r[\"H\"])*1/exp for r in results]\n",
    "        new_row.append(exp)\n",
    "    else:\n",
    "        #Compute results\n",
    "        new_row = [compute_avg_error(H_gt,r[\"H\"]) for r in results]\n",
    "        #Number of experiments is 1\n",
    "        new_row.append(1)\n",
    "    \n",
    "    #Save the updated results\n",
    "    df.loc[index,:] = new_row\n",
    "    df.to_csv(df_file)\n",
    "    \n",
    "    #Visualize results if required\n",
    "    if visualize:\n",
    "        visualize_results(df, results, stitched_img_gt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "982e98de-cf33-4d8c-92b9-04f19606a52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function allows to design a dropdown menu from which a set of experiments can be selected\n",
    "def dropdown_stats_eventhandler(change):\n",
    "    with output:\n",
    "        selected = change.new\n",
    "        file = os.path.join(BENCHMARK_DIR, os.path.join(selected, FILENAME))\n",
    "        \n",
    "        #Read the results of the chosen set of experiments\n",
    "        df = pd.read_csv(file, index_col = INDEX)\n",
    "\n",
    "        #Print the results \n",
    "        visualize_stats(df)\n",
    "        IPython.display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a4fc1a-f785-4d85-842b-f824bf5ff153",
   "metadata": {},
   "source": [
    "## Visualize Statistics\n",
    "It is useful to evaluate performances of methods in a dataset without computing a new benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73e4f7f7-d0db-479d-ae43-d68bd564117b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57f9b7aad1c3463e8d51d9603061d98b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Benchmarks:', options=('Helens_0_0.7_30_POINTS', 'mountain_0_0.7_30_POINTS', 'mountain_a…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ffc3e222fd04d899d5a7ebee5c71a0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Find existing set of experiments\n",
    "directories = os.listdir(BENCHMARK_DIR)\n",
    "output = widgets.Output()\n",
    "\n",
    "#Print dropdown menu\n",
    "dropdown_stats = widgets.Dropdown(options = directories, value=None, description='Benchmarks:')\n",
    "dropdown_stats.observe(dropdown_stats_eventhandler, names='value') \n",
    "display(dropdown_stats, output)\n",
    "IPython.display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2219a197-9f31-42d5-96a4-d11b75ad2a25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
