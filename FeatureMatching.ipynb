{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2004e7c9-b3ae-4b48-8cc0-ee5e1acb6d8c",
   "metadata": {},
   "source": [
    "# Feature Matching\n",
    "This notebook contains the code to perform feature matching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3126b3e-cfa2-446c-931d-b966ae7132e2",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1af4778a-c650-445c-8464-5401488b2d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import networkx as nx\n",
    "from IPython.display import Audio, display\n",
    "import ipynb.fs.defs.Utils as Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4be1e5-dc08-4f70-9339-ef2b7f4db8af",
   "metadata": {},
   "source": [
    "## Classes definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17d78987-9a8a-4dbe-ab83-1566030debd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This class contains the methods and properties related to the features of an image\n",
    "class ImageFeature:\n",
    "    def __init__(self, image, index):\n",
    "        self.image = image\n",
    "        self.index = index  \n",
    "        \n",
    "    #Compute the salient points of the image using SIFT algorithm\n",
    "    def SIFT(self,\n",
    "             save_output = False, #If True saves the output\n",
    "             output_dir = \"output\", #Output directory\n",
    "             noise_type = None,\n",
    "             noise_std = 0.0\n",
    "            ):\n",
    "        sift = cv.SIFT_create()\n",
    "        self.kp, self.des = sift.detectAndCompute(self.image,None)\n",
    "        if noise_type == Utils.NoiseType.POINTS:\n",
    "            for i in range(len(self.kp)):\n",
    "                self.kp[i].pt += np.random.normal(scale = noise_std , size = (2,))\n",
    "        self.img_with_sift=cv.drawKeypoints(self.image,self.kp,np.copy(self.image))\n",
    "        if save_output:\n",
    "            cv.imwrite(os.path.join(output_dir,f\"{self.index+1}.jpg\"), self.img_with_sift)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9e1ce70-ef10-43c4-a83b-a6d8710f9f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This class contains the methods and properties related to the problem of feature matching between two images\n",
    "class Match:\n",
    "    def __init__(self,\n",
    "                 image_feature_source, #Source image\n",
    "                 image_feature_destination #Destination image\n",
    "                ):\n",
    "        self.image_feature_source = image_feature_source\n",
    "        self.image_feature_destination = image_feature_destination\n",
    "    \n",
    "    #This function performs feature matching between to images\n",
    "    def feature_matching(self, threshold, save_output = False, output_dir = \"output\"):\n",
    "        # FLANN parameters\n",
    "        FLANN_INDEX_KDTREE = 1\n",
    "        index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "        search_params = dict(checks=50)\n",
    "        flann = cv.FlannBasedMatcher(index_params,search_params)\n",
    "        \n",
    "        bf = cv.BFMatcher()\n",
    "        matches = bf.knnMatch(self.image_feature_source.des,\n",
    "                                 self.image_feature_destination.des,\n",
    "                                 k=2)\n",
    "        # Need to draw only good matches, so create a mask\n",
    "        self.good = []\n",
    "        for m in matches:\n",
    "            if m[0].distance < threshold*m[1].distance:\n",
    "                self.good.append((m[0],m[0]))\n",
    "        draw_params = dict(matchColor = (0,255,0),\n",
    "                           singlePointColor = (255,0,0),\n",
    "                           flags = cv.DrawMatchesFlags_DEFAULT)\n",
    "        self.img_with_match = cv.drawMatchesKnn(self.image_feature_source.image,\n",
    "                                                self.image_feature_source.kp,\n",
    "                                                self.image_feature_destination.image,\n",
    "                                                self.image_feature_destination.kp,\n",
    "                                                self.good,None,**draw_params)\n",
    "        \n",
    "        self.num_matches = len(self.good)\n",
    "        \n",
    "        #Compute source and destination points\n",
    "        self.src_pts = np.double([self.image_feature_source.kp[m[0].queryIdx].pt for m in self.good])\n",
    "        self.dst_pts = np.double([self.image_feature_destination.kp[m[0].trainIdx].pt for m in self.good])\n",
    "        \n",
    "        #Save the output if required\n",
    "        if save_output:\n",
    "            cv.imwrite(os.path.join(output_dir,f\"match_{self.image_feature_source.index+1}_{ self.image_feature_destination.index+1}.jpg\"),self.img_with_match)\n",
    "    \n",
    "    #This function allows to check whether the match between the source and destination images is to be considered a good match\n",
    "    def check_salient(self, threshold, save_output = False, output_dir = \"output\"):\n",
    "        #A threshold is used to define the goodness of the  match\n",
    "        if self.num_matches > threshold:\n",
    "            if save_output:\n",
    "                cv.imwrite(os.path.join(output_dir,f\"salient_{self.image_feature_source.index+1}_{ self.image_feature_destination.index+1}.jpg\"),self.img_with_match)\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    #This function allows to compute and normalize the projected points\n",
    "    def mult_and_norm(self,H, points):\n",
    "        proj_p = np.dot(H,points)\n",
    "        proj_p = proj_p / proj_p[2,:]\n",
    "        return proj_p[0:2,:]\n",
    "    \n",
    "    #This function allows to estimate the homography between the two images\n",
    "    def fit_homography(self,\n",
    "                       RANSACmaxIters = 2000, #Number of iterations of RANSAC\n",
    "                       T_norm = np.eye(3), #Normalization matrix\n",
    "                       noise_type = Utils.NoiseType.NO_NOISE, #If True introduces some noise in the matches\n",
    "                       noise_std = 0.1  #Standare deviation of the Gaussian noise\n",
    "                      ):\n",
    "        \n",
    "        \n",
    "        self.src_pts = np.concatenate((self.src_pts, np.ones([self.src_pts.shape[0],1])), axis=1)\n",
    "        self.dst_pts = np.concatenate((self.dst_pts, np.ones([self.dst_pts.shape[0],1])), axis=1)\n",
    "        \n",
    "        src_pts_proj = self.mult_and_norm(T_norm,self.src_pts.transpose()).transpose() #Compute normalized source points\n",
    "        dst_pts_proj = self.mult_and_norm(T_norm,self.dst_pts.transpose()).transpose() #Compute normalized destination points\n",
    "        \n",
    "        #Compute the homography from source image to destination image\n",
    "        self.M, _ = cv.findHomography(src_pts_proj, dst_pts_proj, cv.RANSAC, 3*T_norm[0][0], 3.0, maxIters=RANSACmaxIters )\n",
    "\n",
    "        #If required add some noise to the image\n",
    "        if noise_type == Utils.NoiseType.HOMOGRAPHY:\n",
    "            self.M += np.random.normal(scale = noise_std , size = [3,3]) #Add some random noise to the homography\n",
    "    \n",
    "    #This function allows to normalize the homography so that the determinant is unitary\n",
    "    #This makes homographies part of the SL(3) group\n",
    "    def normalize_homography(self):\n",
    "        det = np.linalg.det(self.M)\n",
    "        self.H = self.M/np.cbrt(det)\n",
    "        new_det = np.linalg.det(self.H)\n",
    "        \n",
    "    def copy(self):\n",
    "        copy = Match(self.image_feature_source,\n",
    "                    self.image_feature_destination )\n",
    "        copy.img_with_match = self.img_with_match\n",
    "        copy.good = self.good\n",
    "        copy.num_matches = self.num_matches\n",
    "        copy.src_pts = np.copy(self.src_pts)\n",
    "        copy.dst_pts = np.copy(self.dst_pts)\n",
    "        return copy\n",
    "    \n",
    "    def get_inv_match(self):\n",
    "        inv_match = Match(self.image_feature_destination, self.image_feature_source)\n",
    "        #temp_good = self.good.copy()\n",
    "        #temp_trainIdx = [m[0].trainIdx for m in temp_good]\n",
    "        #for i,m in enumerate(self.good):\n",
    "        #    m[0].trainIdx = m[0].queryIdx\n",
    "        #    m[0].queryIdx = temp_trainIdx[i]\n",
    "        #inv_match.good = temp_good\n",
    "        inv_match.num_matches = self.num_matches\n",
    "        inv_match.dst_pts = np.copy(self.src_pts)\n",
    "        inv_match.src_pts = np.copy(self.dst_pts)\n",
    "        return inv_match\n",
    "    #Remember: if src is image I and dest is image J, we are finding H from I to J, this is called Hj,i (TODO)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73162dc2-8bde-4ed3-9f15-1a4f88b79bb7",
   "metadata": {},
   "source": [
    "## Functions definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5365d9e3-37ec-440f-a955-b840c05a1c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO delete\n",
    "def allDone():\n",
    "    print(\" --- finished --- \")\n",
    "    #display(Audio(url='https://sound.peal.io/ps/audios/000/001/131/original/kon.wav', autoplay=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47b0e6bb-1c10-4048-a1e6-50a1f12fc437",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function allows to compute matches between two images\n",
    "def compute_matches(imgs, #Images for which matches should be computed\n",
    "                    T_norm, #Normalization matrix\n",
    "                    save_images = False, #If True saves the intermediate results\n",
    "                    matching_threshold = 0.6, #Threshold used to compute matches\n",
    "                    matches_dir = \"matches\", #Directory where to save matches\n",
    "                    matches_th = 20, #Threshold used to compute good matches\n",
    "                    number_of_matches=1, #Number of times the matching procedure should be repeated\n",
    "                    noise_std = 0.1, #Standard deviation of the gaussian noise\n",
    "                    salient_matches_dir = \"salient_matches\", #Directory where to save good matches\n",
    "                    sift_dir = \"sift\", #Directory where to save SIFT results\n",
    "                    RANSACmaxIters = 2000, #Number of RANSAC iterations\n",
    "                    noise_type = Utils.NoiseType.NO_NOISE #If True allows to introduce some noise in the homographies\n",
    "                   ):\n",
    "    n = len(imgs)\n",
    "    image_features =[] #Array that will contain the salient features of each image\n",
    "    \n",
    "    #Compute salient points of each image using SIFT\n",
    "    for i in range(0,n):\n",
    "        for k in range(0,number_of_matches):\n",
    "            image_feature = ImageFeature(imgs[i], i)\n",
    "            image_feature.SIFT(save_images, sift_dir, noise_type = noise_type, noise_std = noise_std)\n",
    "            if len(image_features) <= i:\n",
    "                image_features.append([image_feature])\n",
    "            else:\n",
    "                image_features[i].append(image_feature)\n",
    "    #Compute the good matches between each pair of images\n",
    "    matches =[]\n",
    "    for k in range(0,number_of_matches):\n",
    "        for i in range(0, n-1):\n",
    "            for j in range(i+1, n):\n",
    "                match = Match(image_features[i][k], image_features[j][k])\n",
    "                match.feature_matching(matching_threshold, save_images, matches_dir)            \n",
    "                matches.append(match)\n",
    "    \n",
    "    salient_matches = list(filter(lambda x: x.check_salient(matches_th, save_images, salient_matches_dir),matches))\n",
    "    \n",
    "    #For every good match, compute also the matches in the reverse order (destination, source) and compute homographies\n",
    "    total_matches = []\n",
    "    for match in salient_matches:\n",
    "        inv_match = match.get_inv_match()#Match(match.image_feature_destination, match.image_feature_source)\n",
    "        #inv_match.feature_matching(matching_threshold, save_images, matches_dir)\n",
    "        \n",
    "        match.fit_homography(RANSACmaxIters, T_norm, noise_type = noise_type, noise_std = noise_std)\n",
    "        match.normalize_homography()\n",
    "        total_matches.append(match)\n",
    "        \n",
    "        inv_match.fit_homography(RANSACmaxIters, T_norm, noise_type = noise_type, noise_std = noise_std)\n",
    "        inv_match.normalize_homography()\n",
    "        total_matches.append(inv_match)\n",
    "        #print(np.linalg.norm(np.eye(3)-match.M@inv_match.M))\n",
    "    return total_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b71fde8-432f-41b9-bdc5-26a1d5639b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function allows to perform the whole feature matching procedure\n",
    "def get_feature_matches(dataset_name, #Name of the dataset to be used\n",
    "                imgs, #Images used to compute matches\n",
    "                T_norm, #Normalization matrix\n",
    "                matching_threshold = 0.6, #Treshold used to compute matches\n",
    "                number_of_matches = 1, #Number of times the matching procedure should be repeated\n",
    "                noise_std = 0.1, #Standard deviation of the Gaussian noise\n",
    "                matches_th = 20, #Treshold used to compute good matches\n",
    "                RANSACmaxIters = 2000, #Number of RANSAC iterations\n",
    "                save_images = True, #If True saves intermediate results\n",
    "                save_output = True, #If True saves output\n",
    "                output_dir =\"output\", #Output directory\n",
    "                results_dir = \"results\", #Intermediate results directory\n",
    "                noise_type = Utils.NoiseType.NO_NOISE, #If True add some Gaussian noise to homographies\n",
    "                verbose = True #If True prints intermediate results\n",
    "               ):\n",
    "        \n",
    "        #Define directories\n",
    "        sift_dir = os.path.join(results_dir,'sift')\n",
    "        matches_dir = os.path.join(results_dir,'matches')\n",
    "        salient_matches_dir = os.path.join(results_dir,'salient_matches')\n",
    "        Weight_filename = f\"W_{dataset_name}.npy\"\n",
    "        \n",
    "        #Create directories if not exist\n",
    "        if save_images:\n",
    "            shutil.rmtree(results_dir, ignore_errors = True)\n",
    "            if not os.path.isdir(results_dir):   \n",
    "                os.mkdir(results_dir)\n",
    "            if not os.path.isdir(sift_dir):\n",
    "                os.mkdir(sift_dir)\n",
    "            if not os.path.isdir(matches_dir):\n",
    "                os.mkdir(matches_dir)\n",
    "            if not os.path.isdir(salient_matches_dir):\n",
    "                os.mkdir(salient_matches_dir)   \n",
    "            \n",
    "        n = len(imgs)\n",
    "\n",
    "        #Compute matches\n",
    "        matches = compute_matches(imgs = imgs, \n",
    "                                  T_norm = T_norm,\n",
    "                                  save_images= save_images, \n",
    "                                  matching_threshold = matching_threshold, \n",
    "                                  matches_dir = matches_dir, \n",
    "                                  matches_th = matches_th,\n",
    "                                  number_of_matches = number_of_matches,\n",
    "                                  noise_std = noise_std,\n",
    "                                  salient_matches_dir = salient_matches_dir, \n",
    "                                  sift_dir = sift_dir,\n",
    "                                  RANSACmaxIters = RANSACmaxIters,\n",
    "                                  noise_type = noise_type\n",
    "                                 )\n",
    "        \n",
    "        #Create matches dictionary and weight matrix\n",
    "        matches_dict = dict()\n",
    "        weight_matrix = np.zeros([n,n])\n",
    "        for match in matches:\n",
    "            i,j = match.image_feature_source.index, match.image_feature_destination.index\n",
    "            if (i, j) not in matches_dict:\n",
    "                matches_dict[i, j] = list()\n",
    "            matches_dict[i, j].append(match.H)\n",
    "            weight_matrix[i, j] = match.num_matches\n",
    "        \n",
    "        #If required save output\n",
    "        if save_output:\n",
    "            if not os.path.isdir(output_dir):\n",
    "                os.makedirs(output_dir)\n",
    "            np.save(os.path.join(output_dir,Weight_filename), weight_matrix)\n",
    "\n",
    "        if verbose:#TODO remove\n",
    "            allDone()\n",
    "        \n",
    "        return matches_dict, weight_matrix\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
